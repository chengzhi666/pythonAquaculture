# 项目文档合并

## 项目简介

### 项目名称
**pythonAquaculture** - 水产情报采集 MVP

**目标**：将多来源采集统一成一套流程：`跑批 -> 入 MySQL -> 可回溯 -> 可去重`。

### 系统架构

#### 系统一：Streamlit WebUI（默认 MySQL）
- **入口**：`app.py`
- **数据库**：默认 MySQL（可通过 `STORAGE_BACKEND=sqlite` 切换到本地 SQLite）
- **用途**：快速展示、交互式采集、情报检索
- **爬虫模块**：`crawlers/` 目录下的爬虫
- **运行方式**：`streamlit run app.py`

#### 系统二：批量处理系统（MySQL）
- **入口**：`fish_intel_mvp/run_one.py`
- **数据库**：MySQL（需要预先配置和创建库表）
- **用途**：生产级批量数据采集、日志审计、完整数据溯源
- **爬虫模块**：`fish_intel_mvp/jobs/` 目录下的爬虫
- **运行方式**：`python fish_intel_mvp/run_one.py <job>`

### 当前状态
- **`jd`**：已可运行，入表 `product_snapshot`，含 `raw_event` 证据。
- **`moa`**：已可运行，入表 `intel_item`，含 `raw_event` 证据。
- **`cnki`**：已接入 `paper_meta`，可运行但依赖本机 Selenium/Edge 驱动环境。
- **`taobao`**：已实现（基于淘宝 H5 接口抓取，需要有效 `TAOBAO_COOKIE`）。

---

## 开发环境设置

### 1. 创建虚拟环境

```powershell
python -m venv .venv
.\.venv\Scripts\activate
```

### 2. 安装依赖

```powershell
# 安装基础依赖
pip install -r fish_intel_mvp\requirements.txt

# 或使用 pyproject.toml（推荐）
pip install -e ".[dev]"
```

### 3. 本地配置

```powershell
# 复制本地开发配置（可选）
copy .env.local.example .env.local
```

### 代码质量工具

#### 代码格式化（Black）

```powershell
black .
```

#### 代码检查（Flake8）

```powershell
flake8 crawlers/ storage/ query/ --max-line-length=100
```

#### 类型检查（MyPy）

```powershell
mypy crawlers/ storage/ query/ --ignore-missing-imports
```

#### 代码分析（Pylint）

```powershell
pylint crawlers/ storage/ query/
```

---

## 优化细节

### 📋 本次优化概览

本次优化涉及 6 个主要方面，共计 10+ 项改进，提升代码质量、性能和可维护性。

### 1. 数据库操作优化 ⚡

#### 问题
- ❌ SQLite `save_items()` 逐条插入，每条都提交一次
- ❌ 没有事务管理，一旦失败所有更改回滚
- ❌ 没有日志记录和错误处理

#### 优化方案

```python
# 改进前：10 条记录 = 10 次提交
for it in items:
    cur.execute(...)
conn.commit()  # ← 10 次

# 改进后：10 条记录 = 1 次提交
cur.execute("BEGIN TRANSACTION")
for it in items:
    cur.execute(...)
conn.commit()  # ← 1 次
```

#### 效果
- **性能提升**：单条记录插入速度快 3-5 倍
- **原子性保证**：all-or-nothing，避免部分失败
- **错误恢复**：自动 rollback 防止数据损坏
- **可观测性**：添加了详细日志记录

#### 文件
- `storage/db.py` - 优化后的批量插入和事务管理

---

### 2. 日志系统统一化 📝

#### 问题
- ❌ SQLite 系统完全没有日志
- ❌ 爬虫错误处理不一致

#### 优化方案
- 引入统一的日志记录模块 `logger.py`
- 日志分级：INFO、WARNING、ERROR
- 爬虫错误自动记录到 `raw_event` 表

#### 效果
- **可追溯性**：所有操作都有日志记录
- **错误分析**：爬虫失败原因可追踪

#### 文件
- `common/logger.py` - 统一日志模块

---

## 目录结构

```text
pythonAquaculture/
  app.py                        # Streamlit WebUI 入口
  runner.py                     # 配置驱动的采集框架
  config/
    sites.json                  # 采集源配置文件
  crawlers/                     # 爬虫模块（供 Streamlit / runner 使用）
    cnki_crawler.py
    moa_fishery_crawler.py
    scholar_crawler.py
  storage/
    db.py                       # 统一存储层（默认 MySQL，兼容 SQLite）
  fish_intel_mvp/               # 批量处理系统（MySQL）
    jobs/                       # 爬虫任务
      crawl_taobao.py
      crawl_jd.py
      crawl_moa_fishery.py
      crawl_cnki.py
    schema.sql                  # 数据库表结构定义
    requirements.txt            # 项目依赖
    run_one.py                  # 批量任务入口
  tests/                        # 测试用例
    test_config_storage.py
    test_storage.py
```

---

## 合并说明
- **DEVELOPMENT.md**：开发环境设置部分已合并到“开发环境设置”章节。
- **OPTIMIZATIONS.md**：优化细节部分已合并到“优化细节”章节。
- **README.md**：项目简介和目录结构部分已合并到对应章节。
- **VS CODE SETUP.md**：文件不存在，未合并。